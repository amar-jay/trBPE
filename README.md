## trBPE

The current landscape of LLMs predominantly caters to the English language. This bias can be attributed to two primary factors: extensive training on English datasets and the efficacy of token embedding. Notably, GPT4's token embedding stands out as one of the most advanced in recent times. Its superiority lies in its ability to contextualize tokens based on syllabic divisions, enhancing comprehension and generation capabilities.

However, for foreign languages, especially Turkish, this advantage diminishes due to the inherent randomness in tokenization. Consequently, there is a pressing need to address this limitation, prompting the creation of this repository. Our goal is to develop a BPE tokenizer specifically tailored to Turkish, leveraging datasets rich in Turkish language content.

Our aspiration is to surpass the tokenization quality achieved by GPT4's BPE in the Turkish language domain.
